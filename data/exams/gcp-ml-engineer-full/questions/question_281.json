{
  "id": "question_281",
  "number": 281,
  "topic": "Topic 1",
  "text": "You work at a large organization that recently decided to move their ML and data workloads to Google Cloud. The data engineering team has exported the structured data to a Cloud Storage bucket in Avro format. You need to propose a workflow that performs analytics, creates features, and hosts the features that your ML models use for online prediction. How should you configure the pipeline?",
  "choices": [
    {
      "letter": "A",
      "text": "Ingest the Avro files into Cloud Spanner to perform analytics. Use a Dataflow pipeline to create the features, and store them in Vertex AI Feature Store for online prediction.",
      "is_most_voted": false,
      "is_correct": false
    },
    {
      "letter": "B",
      "text": "Ingest the Avro files into BigQuery to perform analytics. Use a Dataflow pipeline to create the features, and store them in Vertex AI Feature Store for online prediction.",
      "is_most_voted": true,
      "is_correct": true
    },
    {
      "letter": "C",
      "text": "Ingest the Avro files into Cloud Spanner to perform analytics. Use a Dataflow pipeline to create the features, and store them in BigQuery for online prediction.",
      "is_most_voted": false,
      "is_correct": false
    },
    {
      "letter": "D",
      "text": "Ingest the Avro files into BigQuery to perform analytics. Use BigQuery SQL to create features and store them in a separate BigQuery table for online prediction.",
      "is_most_voted": false,
      "is_correct": false
    }
  ],
  "correct_answer": "B",
  "explanation": "",
  "voting_data": {
    "total_votes": 14,
    "vote_distribution": {},
    "most_voted_answer": "B",
    "confidence_score": 0.9285714285714286
  },
  "metadata": {
    "extraction_timestamp": "2025-05-29T12:25:19.658471",
    "source_url": "data/input/page-6.html",
    "page_number": 1,
    "difficulty_level": ""
  }
}
