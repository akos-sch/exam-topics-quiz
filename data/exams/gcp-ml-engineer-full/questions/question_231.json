{
  "id": "question_231",
  "number": 231,
  "topic": "Topic 1",
  "text": "You are using Keras and TensorFlow to develop a fraud detection model. Records of customer transactions are stored in a large table in BigQuery. You need to preprocess these records in a cost-effective and efficient way before you use them to train the model. The trained model will be used to perform batch inference in BigQuery. How should you implement the preprocessing workflow?",
  "choices": [
    {
      "letter": "A",
      "text": "Implement a preprocessing pipeline by using Apache Spark, and run the pipeline on Dataproc. Save the preprocessed data as CSV files in a Cloud Storage bucket.",
      "is_most_voted": false,
      "is_correct": false
    },
    {
      "letter": "B",
      "text": "Load the data into a pandas DataFrame. Implement the preprocessing steps using pandas transformations, and train the model directly on the DataFrame.",
      "is_most_voted": false,
      "is_correct": false
    },
    {
      "letter": "C",
      "text": "Perform preprocessing in BigQuery by using SQL. Use the BigQueryClient in TensorFlow to read the data directly from BigQuery.",
      "is_most_voted": false,
      "is_correct": true
    },
    {
      "letter": "D",
      "text": "Implement a preprocessing pipeline by using Apache Beam, and run the pipeline on Dataflow. Save the preprocessed data as CSV files in a Cloud Storage bucket.",
      "is_most_voted": false,
      "is_correct": false
    }
  ],
  "correct_answer": "C",
  "explanation": "",
  "voting_data": {
    "total_votes": 11,
    "vote_distribution": {},
    "most_voted_answer": "C",
    "confidence_score": 1.0
  },
  "metadata": {
    "extraction_timestamp": "2025-05-29T12:21:28.671013",
    "source_url": "data/input/page-5.html",
    "page_number": 1,
    "difficulty_level": ""
  }
}
