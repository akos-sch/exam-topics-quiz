{
  "id": "question_198",
  "number": 198,
  "topic": "Topic 1",
  "text": "You developed a Transformer model in TensorFlow to translate text. Your training data includes millions of documents in a Cloud Storage bucket. You plan to use distributed training to reduce training time. You need to configure the training job while minimizing the effort required to modify code and to manage the clusterâ€™s configuration. What should you do?",
  "choices": [
    {
      "letter": "A",
      "text": "Create a Vertex AI custom training job with GPU accelerators for the second worker pool. Use tf.distribute.MultiWorkerMirroredStrategy for distribution.",
      "is_most_voted": true,
      "is_correct": true
    },
    {
      "letter": "B",
      "text": "Create a Vertex AI custom distributed training job with Reduction Server. Use N1 high-memory machine type instances for the first and second pools, and use N1 high-CPU machine type instances for the third worker pool.",
      "is_most_voted": false,
      "is_correct": false
    },
    {
      "letter": "C",
      "text": "Create a training job that uses Cloud TPU VMs. Use tf.distribute.TPUStrategy for distribution.",
      "is_most_voted": false,
      "is_correct": false
    },
    {
      "letter": "D",
      "text": "Create a Vertex AI custom training job with a single worker pool of A2 GPU machine type instances. Use tf.distribute.MirroredStrategv for distribution.",
      "is_most_voted": false,
      "is_correct": false
    }
  ],
  "correct_answer": "A",
  "explanation": "",
  "voting_data": {
    "total_votes": 8,
    "vote_distribution": {},
    "most_voted_answer": "A",
    "confidence_score": 1.0
  },
  "metadata": {
    "extraction_timestamp": "2025-05-29T12:19:01.346671",
    "source_url": "data/input/page-4.html",
    "page_number": 1,
    "difficulty_level": ""
  }
}
