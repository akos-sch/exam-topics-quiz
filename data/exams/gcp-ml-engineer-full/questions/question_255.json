{
  "id": "question_255",
  "number": 255,
  "topic": "Topic 1",
  "text": "You have recently used TensorFlow to train a classification model on tabular data. You have created a Dataflow pipeline that can transform several terabytes of data into training or prediction datasets consisting of TFRecords. You now need to productionize the model, and you want the predictions to be automatically uploaded to a BigQuery table on a weekly schedule. What should you do?",
  "choices": [
    {
      "letter": "A",
      "text": "Import the model into Vertex AI and deploy it to a Vertex AI endpoint. On Vertex AI Pipelines, create a pipeline that uses the DataflowPythonJobOp and the ModelBacthPredictOp components.",
      "is_most_voted": false,
      "is_correct": false
    },
    {
      "letter": "B",
      "text": "Import the model into Vertex AI and deploy it to a Vertex AI endpoint. Create a Dataflow pipeline that reuses the data processing logic sends requests to the endpoint, and then uploads predictions to a BigQuery table.",
      "is_most_voted": false,
      "is_correct": false
    },
    {
      "letter": "C",
      "text": "Import the model into Vertex AI. On Vertex AI Pipelines, create a pipeline that uses the<br/>DataflowPvthonJobOp and the ModelBatchPredictOp components.",
      "is_most_voted": true,
      "is_correct": true
    },
    {
      "letter": "D",
      "text": "Import the model into BigQuery. Implement the data processing logic in a SQL query. On Vertex AI Pipelines create a pipeline that uses the BigquervQueryJobOp and the BigqueryPredictModelJobOp components.",
      "is_most_voted": false,
      "is_correct": false
    }
  ],
  "correct_answer": "C",
  "explanation": "",
  "voting_data": {
    "total_votes": 30,
    "vote_distribution": {},
    "most_voted_answer": "C",
    "confidence_score": 0.6
  },
  "metadata": {
    "extraction_timestamp": "2025-05-29T12:23:27.267338",
    "source_url": "data/input/page-6.html",
    "page_number": 1,
    "difficulty_level": ""
  }
}
